#!/usr/bin/env python
# -*- coding: UTF-8 -*-
#################################################################################
# Copyright 2018 ROBOTIS CO., LTD.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#################################################################################

# Authors: Gilbert #

import rospy
import os
import json
import numpy as np
import random
import time
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray
from src.turtlebot3_dqn.environment_stage_4 import Env
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf


EPISODES = 3000

#####################  hyper parameters  ####################

RANDOMSEED = 1              # random seed

LR_A = 0.00025                # learning rate for actor
LR_C = 0.0005              # learning rate for critic
GAMMA = 0.9                 # reward discount
TAU = 0.01                  # soft replacement
MEMORY_CAPACITY = 10000     # size of replay buffer
BATCH_SIZE = 64             # update batchsize

MAX_EPISODES = 3000          # total number of episodes for training
MAX_EP_STEPS = 500          # total number of steps for each episode
TEST_PER_EPISODES = 10      # test the model per episodes
VAR = 3                     # control exploration

###############################  DDPG  ####################################

# tf.enable_eager_execution()
class DDPG(object):
    """
    DDPG class
    """

    def __init__(self, action_dim, state_dim, action_range):
        self.memory = np.zeros((MEMORY_CAPACITY, state_dim * 2 + action_dim + 1), dtype=np.float32)
        self.pointer = 0
        self.action_dim, self.state_dim, self.action_range = action_dim, state_dim, action_range
        self.var = VAR

        W_init = tf.random_normal_initializer(mean=0, stddev=0.3)
        b_init = tf.constant_initializer(0.1)

        def get_actor(input_state_shape, name=''):
            """
            Build actor network
            :param input_state_shape: state
            :param name: name
            :return: act
            """
            input_layer = tl.layers.Input(input_state_shape, name='A_input')
            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='A_l1')(input_layer)
            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='A_l2')(layer)
            layer = tl.layers.Dense(n_units=action_dim, act=tf.nn.tanh, W_init=W_init, b_init=b_init, name='A_a')(layer)
            layer = tl.layers.Lambda(lambda x: action_range * x)(layer)
            return tl.models.Model(inputs=input_layer, outputs=layer, name='Actor' + name)

        def get_critic(input_state_shape, input_action_shape, name=''):
            """
            Build critic network
            :param input_state_shape: state
            :param input_action_shape: act
            :param name: name
            :return: Q value Q(s,a)
            """
            state_input = tl.layers.Input(input_state_shape, name='C_s_input')
            action_input = tl.layers.Input(input_action_shape, name='C_a_input')
            layer = tl.layers.Concat(1)([state_input, action_input])
            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='C_l1')(layer)
            layer = tl.layers.Dense(n_units=64, act=tf.nn.relu, W_init=W_init, b_init=b_init, name='C_l2')(layer)
            layer = tl.layers.Dense(n_units=1, W_init=W_init, b_init=b_init, name='C_out')(layer)
            return tl.models.Model(inputs=[state_input, action_input], outputs=layer, name='Critic' + name)

        self.actor = get_actor([None, state_dim])
        self.critic = get_critic([None, state_dim], [None, action_dim])
        self.actor.train()
        self.critic.train()

        def copy_para(from_model, to_model):
            """
            Copy parameters for soft updating
            :param from_model: latest model
            :param to_model: target model
            :return: None
            """
            for i, j in zip(from_model.trainable_weights, to_model.trainable_weights):
                j.assign(i)

        self.actor_target = get_actor([None, state_dim], name='_target')
        copy_para(self.actor, self.actor_target)
        self.actor_target.eval()

        self.critic_target = get_critic([None, state_dim], [None, action_dim], name='_target')
        copy_para(self.critic, self.critic_target)
        self.critic_target.eval()

        self.ema = tf.train.ExponentialMovingAverage(decay=1 - TAU)  # soft replacement

        self.actor_opt = tf.optimizers.Adam(LR_A)
        self.critic_opt = tf.optimizers.Adam(LR_C)

    def ema_update(self):
        """
        Soft updating by exponential smoothing
        :return: None
        """
        paras = self.actor.trainable_weights + self.critic.trainable_weights
        self.ema.apply(paras)
        for i, j in zip(self.actor_target.trainable_weights + self.critic_target.trainable_weights, paras):
            i.assign(self.ema.average(j))

    def get_action(self, s, greedy=False):
        """
        Choose action
        :param s: state
        :param greedy: get action greedy or not
        :return: act
        """
        a = self.actor(np.array([s], dtype=np.float32))[0]
        if greedy:
            return a
        return np.clip(
            np.random.normal(a, self.var), -self.action_range, self.action_range
        )  # add randomness to action selection for exploration

    def learn(self):
        """
        Update parameters
        :return: None
        """
        self.var *= .9995
        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)
        datas = self.memory[indices, :]
        states = datas[:, :self.state_dim]
        actions = datas[:, self.state_dim:self.state_dim + self.action_dim]
        rewards = datas[:, -self.state_dim - 1:-self.state_dim]
        states_ = datas[:, -self.state_dim:]

        with tf.GradientTape() as tape:
            actions_ = self.actor_target(states_)
            q_ = self.critic_target([states_, actions_])
            y = rewards + GAMMA * q_
            q = self.critic([states, actions])
            td_error = tf.losses.mean_squared_error(y, q)
        critic_grads = tape.gradient(td_error, self.critic.trainable_weights)
        self.critic_opt.apply_gradients(zip(critic_grads, self.critic.trainable_weights))

        with tf.GradientTape() as tape:
            a = self.actor(states)
            q = self.critic([states, a])
            actor_loss = -tf.reduce_mean(q)  # maximize the q
        actor_grads = tape.gradient(actor_loss, self.actor.trainable_weights)
        self.actor_opt.apply_gradients(zip(actor_grads, self.actor.trainable_weights))
        self.ema_update()

    def store_transition(self, s, a, r, s_):
        """
        Store data in data buffer
        :param s: state
        :param a: act
        :param r: reward
        :param s_: next state
        :return: None
        """
        s = s.astype(np.float32)
        s_ = s_.astype(np.float32)
        transition = np.hstack((s, a, [r], s_))
        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory
        self.memory[index, :] = transition
        self.pointer += 1

    def save(self):
        """
        save trained weights
        :return: None
        """
        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))
        if not os.path.exists(path):
            os.makedirs(path)
        tl.files.save_weights_to_hdf5(os.path.join(path, 'actor.hdf5'), self.actor)
        tl.files.save_weights_to_hdf5(os.path.join(path, 'actor_target.hdf5'), self.actor_target)
        tl.files.save_weights_to_hdf5(os.path.join(path, 'critic.hdf5'), self.critic)
        tl.files.save_weights_to_hdf5(os.path.join(path, 'critic_target.hdf5'), self.critic_target)

    def load(self):
        """
        load trained weights
        :return: None
        """
        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))
        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor.hdf5'), self.actor)
        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'actor_target.hdf5'), self.actor_target)
        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic.hdf5'), self.critic)
        tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'critic_target.hdf5'), self.critic_target)

if __name__ == '__main__':

    rospy.init_node('turtlebot3_dqn_stage_4')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    get_action = Float32MultiArray()

    state_size = 28
    action_size = 5

    # 初始化环境
    env = Env(action_size)

    # 定义状态空间，动作空间，动作幅度范围
    s_dim = state_size
    a_dim = action_size

    print('s_dim', s_dim)
    print('a_dim', a_dim)

    # 用DDPG算法
    ddpg = DDPG(a_dim, s_dim)

    # 训练部分：
    # train

    reward_buffer = []  # 用于记录每个EP的reward，统计变化
    t0 = time.time()  # 统计时间
    for i in range(MAX_EPISODES):
        done = False
        t1 = time.time()
        s = env.reset()
        ep_reward = 0  # 记录当前EP的reward
        for j in range(MAX_EP_STEPS):
            # Add exploration noise
            a = ddpg.choose_action(s)  # 这里很简单，直接用actor估算出a动作

            # 为了能保持开发，这里用了另外一种方式增加探索。
            # 因此需要需要以a为均值，VAR为标准差，建立正态分布，再从正态分布采样出a
            # 因为a是均值，所以a的概率是最大的。但a相对其他概率由多大，是靠VAR调整。这里我们其实可以增加更新VAR，动态调整a的确定性
            # 然后进行裁剪
            a = np.clip(np.random.normal(a, VAR), -2, 2)

            action = np.argmax(a)
            # 与环境进行互动
            s_, r, done= env.step(action)

            # 保存s，a，r，s_
            ddpg.store_transition(s, a, r / 10, s_)

            # 第一次数据满了，就可以开始学习
            if ddpg.pointer > MEMORY_CAPACITY:
                ddpg.learn()

            # 输出数据记录
            s = s_
            ep_reward += r  # 记录当前EP的总reward

            get_action.data = [action, ep_reward, r]
            pub_get_action.publish(get_action)

            if i % 10 == 0:
                ddpg.save_ckpt(i)

            if j == MAX_EP_STEPS - 1 or done:
                print(
                    '\rTrain:Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'.format(
                        i, MAX_EPISODES, ep_reward,
                        time.time() - t1
                    )
                )
            # plt.show()
            if done:
                break  # collision restart

        result.data = [ep_reward, r]
        pub_result.publish(result)

        # test
        if i and not i % TEST_PER_EPISODES:
            t1 = time.time()
            s = env.reset()
            ep_reward = 0
            for j in range(MAX_EP_STEPS):

                a = ddpg.choose_action(s)  # 注意，在测试的时候，我们就不需要用正态分布了，直接一个a就可以了。
                action = np.argmax(a)
                s_, r, done = env.step(action)

                s = s_
                ep_reward += r
                if j == MAX_EP_STEPS - 1 or done:
                    print(
                        '\rTest:Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'.format(
                            i, MAX_EPISODES, ep_reward,
                            time.time() - t1
                        )
                    )

                    reward_buffer.append(ep_reward)

                if done:
                    break

    #     if reward_buffer:
    #         plt.ion()
    #         plt.cla()
    #         plt.title('DDPG')
    #         plt.plot(np.array(range(len(reward_buffer))) * TEST_PER_EPISODES, reward_buffer)  # plot the episode vt
    #         plt.xlabel('episode steps')
    #         plt.ylabel('normalized state-action value')
    #         plt.ylim(-2000, 0)
    #         plt.show()
    #         plt.pause(0.1)
    # plt.ioff()
    # plt.show()
    print('\nRunning time: ', time.time() - t0)
    # ddpg.save_ckpt()

    # test
    # ddpg.load_ckpt()
    # while True:
    #     s = env.reset()
    #     for i in range(MAX_EP_STEPS):
    #         env.render()
    #         s, r, done, info = env.step(ddpg.choose_action(s))
    #         if done:
    #             break




    # agent = ReinforceAgent(state_size, action_size)
    # scores, episodes = [], []
    # global_step = 0
    # start_time = time.time()
    #
    # for e in range(agent.load_episode + 1, EPISODES):
    #     done = False
    #     state = env.reset()
    #     score = 0
    #     for t in range(agent.episode_step):
    #         action = agent.getAction(state)
    #
    #         next_state, reward, done = env.step(action)
    #
    #         agent.appendMemory(state, action, reward, next_state, done)
    #
    #         if len(agent.memory) >= agent.train_start:
    #             if global_step <= agent.target_update:
    #                 agent.trainModel()
    #             else:
    #                 agent.trainModel(True)
    #
    #         score += reward
    #         state = next_state
    #         get_action.data = [action, score, reward]
    #         pub_get_action.publish(get_action)
    #
    #         if e % 10 == 0:
    #             agent.model.save(agent.dirPath + str(e) + '.h5')
    #             with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
    #                 json.dump(param_dictionary, outfile)
    #
    #         if t >= 500:
    #             rospy.loginfo("Time out!!")
    #             done = True
    #
    #         if done:
    #             result.data = [score, np.max(agent.q_value)]
    #             pub_result.publish(result)
    #             agent.updateTargetModel()
    #             scores.append(score)
    #             episodes.append(e)
    #             m, s = divmod(int(time.time() - start_time), 60)
    #             h, m = divmod(m, 60)
    #
    #             rospy.loginfo('Ep: %d score: %.2f memory: %d epsilon: %.2f time: %d:%02d:%02d',
    #                           e, score, len(agent.memory), agent.epsilon, h, m, s)
    #             param_keys = ['epsilon']
    #             param_values = [agent.epsilon]
    #             param_dictionary = dict(zip(param_keys, param_values))
    #             break
    #
    #         global_step += 1
    #         if global_step % agent.target_update == 0:
    #             rospy.loginfo("UPDATE TARGET NETWORK")
    #
    #     if agent.epsilon > agent.epsilon_min:
    #         agent.epsilon *= agent.epsilon_decay